{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d528be77",
   "metadata": {},
   "source": [
    "# Max Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1db9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from thorr.utils import read_config\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import KFold, ShuffleSplit, RepeatedKFold, train_test_split, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from permetrics.regression import RegressionMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6668505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path('/Users/gdarkwah/Library/CloudStorage/OneDrive-UW/01-Research/03-HAB/.env/hab_config.ini')\n",
    "config_dict = read_config(config_path)\n",
    "project_dir = Path(config_dict[\"project\"][\"project_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424b980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "seed = 1955\n",
    "test_size = 0.2\n",
    "list_metrics = [\"RMSE\", \"MAE\", \"NSE\", \"R2\", \"KGE\", \"MSE\"]\n",
    "\n",
    "# Cross-validation parameters\n",
    "n_splits = 5\n",
    "n_repeats = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fb6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary for tuning results\n",
    "tuning_results = {metric: [] for metric in list_metrics}\n",
    "tuning_results['hyperparameter'] = []\n",
    "tuning_results['hypervalue'] = []\n",
    "tuning_results['mission'] = []\n",
    "tuning_results['set_type'] = []\n",
    "\n",
    "tuning_results_df = pd.DataFrame(tuning_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ed117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_name = 'max_samples'\n",
    "hp_values = np.arange(.1, 1.1, .1)\n",
    "\n",
    "hyperparameters = {\n",
    "    hp_name: hp_values,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b21c7b",
   "metadata": {},
   "source": [
    "## HLS Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325ac64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlsl30_model_name = \"rfr_hlsl30\"\n",
    "\n",
    "hlsl30_features = [\n",
    "    # \"b01_median\",\n",
    "    \"b02_median\",\n",
    "    \"b03_median\",\n",
    "    \"b04_median\",\n",
    "    \"b05_median\",\n",
    "    \"b06_median\",\n",
    "    \"b07_median\",\n",
    "    # \"b09_median\",\n",
    "    \"doy\"\n",
    "]\n",
    "hlsl30_target = \"log_chl_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915707cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_insitu = pd.read_csv(project_dir / 'data/hls_insitu/hls_insitu.csv', low_memory=False)\n",
    "hls_insitu['Date'] = pd.to_datetime(hls_insitu['Date'])\n",
    "hls_insitu['log_chl_a'] = np.log10(hls_insitu['chl_a'])\n",
    "hls_insitu['doy'] = hls_insitu['Date'].dt.dayofyear\n",
    "\n",
    "# filter l30 data and select features + target\n",
    "hlsl30_insitu = hls_insitu[hls_insitu['mission']=='l30'][\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"StationID\",\n",
    "    ] + hlsl30_features + [hlsl30_target]\n",
    "].copy()\n",
    "\n",
    "# remove hls_insitu from memory\n",
    "del hls_insitu\n",
    "\n",
    "hlsl30_insitu.dropna(\n",
    "    subset=hlsl30_features + [hlsl30_target],\n",
    "    inplace=True,\n",
    "    # how=\"all\"\n",
    ")\n",
    "# shuffle l30 data\n",
    "hlsl30_insitu = hlsl30_insitu.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d326b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hp, hp_space in hyperparameters.items():\n",
    "    for value in hp_space:\n",
    "        model = RandomForestRegressor(**{hp: value, 'random_state': seed})\n",
    "\n",
    "        cv_splitter = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "        # cv_train_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_train_results['hyperparameter'] = []\n",
    "        # cv_train_results['hypervalue'] = []\n",
    "        # cv_train_results['set_type'] = []\n",
    "        # cv_train_results['mission'] = []\n",
    "\n",
    "        # cv_val_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_val_results['hyperparameter'] = []\n",
    "        # cv_val_results['hypervalue'] = []\n",
    "        # cv_val_results['set_type'] = []\n",
    "        # cv_val_results['mission'] = []\n",
    "\n",
    "        for train_index, val_index in cv_splitter.split(hlsl30_insitu):\n",
    "            X_train = hlsl30_insitu.iloc[train_index][hlsl30_features]\n",
    "            y_train = hlsl30_insitu.iloc[train_index][hlsl30_target]\n",
    "\n",
    "            X_val = hlsl30_insitu.iloc[val_index][hlsl30_features]\n",
    "            y_val = hlsl30_insitu.iloc[val_index][hlsl30_target]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            train_metrics = RegressionMetric(\n",
    "                y_true=y_train.values,\n",
    "                y_pred=list(y_train_pred)\n",
    "            )\n",
    "\n",
    "            val_metrics = RegressionMetric(\n",
    "                y_true=y_val.values,\n",
    "                y_pred=list(y_val_pred)\n",
    "            )\n",
    "\n",
    "            train_metrics = train_metrics.get_metrics_by_list_names(list_metrics)\n",
    "            val_metrics = val_metrics.get_metrics_by_list_names(list_metrics)\n",
    "\n",
    "            # calculate metrics\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(train_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('train')\n",
    "            tuning_results['mission'].append('l30')\n",
    "\n",
    "\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(val_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('val')\n",
    "            tuning_results['mission'].append('l30')\n",
    "\n",
    "del hlsl30_insitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dbee91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results_df = pd.DataFrame(tuning_results)\n",
    "tuning_results_df.to_csv(f'{hp_name}_results.csv', index=False)\n",
    "\n",
    "del tuning_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8657e",
   "metadata": {},
   "source": [
    "## HLS Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b314b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlss30_model_name = \"rfr_hlss30\"\n",
    "\n",
    "hlss30_features = [\n",
    "    # \"b01_median\",\n",
    "    \"b02_median\",\n",
    "    \"b03_median\",\n",
    "    \"b04_median\",\n",
    "    \"b05_median\",\n",
    "    # \"b06_median\",\n",
    "    \"b07_median\",\n",
    "    # \"b08_median\",\n",
    "    \"b8a_median\",\n",
    "    \"b09_median\",\n",
    "    # \"b10_median\",\n",
    "    # \"b11_median\",\n",
    "    # \"b12_median\",\n",
    "    \"doy\"\n",
    "]\n",
    "hlss30_target = \"log_chl_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dfbd74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_insitu = pd.read_csv(project_dir / 'data/hls_insitu/hls_insitu.csv', low_memory=False)\n",
    "hls_insitu['Date'] = pd.to_datetime(hls_insitu['Date'])\n",
    "hls_insitu['log_chl_a'] = np.log10(hls_insitu['chl_a'])\n",
    "hls_insitu['doy'] = hls_insitu['Date'].dt.dayofyear\n",
    "\n",
    "# filter l30 data and select features + target\n",
    "hlss30_insitu = hls_insitu[hls_insitu['mission']=='s30'][\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"StationID\",\n",
    "    ] + hlss30_features + [hlss30_target]\n",
    "].copy()\n",
    "\n",
    "# remove hls_insitu from memory\n",
    "del hls_insitu\n",
    "\n",
    "hlss30_insitu.dropna(\n",
    "    subset=hlss30_features + [hlss30_target],\n",
    "    inplace=True,\n",
    "    # how=\"all\"\n",
    ")\n",
    "# shuffle l30 data\n",
    "hlss30_insitu = hlss30_insitu.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe15138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>StationID</th>\n",
       "      <th>b02_median</th>\n",
       "      <th>b03_median</th>\n",
       "      <th>b04_median</th>\n",
       "      <th>b05_median</th>\n",
       "      <th>b07_median</th>\n",
       "      <th>b8a_median</th>\n",
       "      <th>b09_median</th>\n",
       "      <th>doy</th>\n",
       "      <th>log_chl_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-22</td>\n",
       "      <td>USGS_05549500</td>\n",
       "      <td>0.018301</td>\n",
       "      <td>0.032668</td>\n",
       "      <td>0.027823</td>\n",
       "      <td>0.033722</td>\n",
       "      <td>0.020919</td>\n",
       "      <td>0.017786</td>\n",
       "      <td>0.024645</td>\n",
       "      <td>81</td>\n",
       "      <td>1.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>21VASWCB-2-JMS157.28</td>\n",
       "      <td>0.030897</td>\n",
       "      <td>0.060766</td>\n",
       "      <td>0.073974</td>\n",
       "      <td>0.071041</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.027774</td>\n",
       "      <td>0.018974</td>\n",
       "      <td>64</td>\n",
       "      <td>0.176091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-22</td>\n",
       "      <td>UMC-7314/13.6</td>\n",
       "      <td>0.016177</td>\n",
       "      <td>0.022933</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>0.013148</td>\n",
       "      <td>0.028313</td>\n",
       "      <td>0.026328</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>203</td>\n",
       "      <td>0.146128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-25</td>\n",
       "      <td>31DELRBC_WQX-892077</td>\n",
       "      <td>0.016124</td>\n",
       "      <td>0.021928</td>\n",
       "      <td>0.020052</td>\n",
       "      <td>0.015140</td>\n",
       "      <td>0.009258</td>\n",
       "      <td>0.007346</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>84</td>\n",
       "      <td>0.600973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-20</td>\n",
       "      <td>21VASWCB-2-JMS110.30</td>\n",
       "      <td>0.014645</td>\n",
       "      <td>0.030327</td>\n",
       "      <td>0.026220</td>\n",
       "      <td>0.049796</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.071667</td>\n",
       "      <td>0.035572</td>\n",
       "      <td>293</td>\n",
       "      <td>-0.107905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7621</th>\n",
       "      <td>2023-07-27</td>\n",
       "      <td>USGS_05586300</td>\n",
       "      <td>0.028349</td>\n",
       "      <td>0.051749</td>\n",
       "      <td>0.041979</td>\n",
       "      <td>0.051625</td>\n",
       "      <td>0.073117</td>\n",
       "      <td>0.064321</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>208</td>\n",
       "      <td>1.287802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7622</th>\n",
       "      <td>2024-06-05</td>\n",
       "      <td>USGS_453027122400000</td>\n",
       "      <td>0.016115</td>\n",
       "      <td>0.027910</td>\n",
       "      <td>0.018125</td>\n",
       "      <td>0.016156</td>\n",
       "      <td>0.020404</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>157</td>\n",
       "      <td>-0.086186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623</th>\n",
       "      <td>2025-07-23</td>\n",
       "      <td>USGS_05474500</td>\n",
       "      <td>0.029470</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.045450</td>\n",
       "      <td>0.060928</td>\n",
       "      <td>0.068560</td>\n",
       "      <td>0.056490</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>204</td>\n",
       "      <td>1.081347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7624</th>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>SCEQ-GL12</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.057943</td>\n",
       "      <td>0.051518</td>\n",
       "      <td>0.064819</td>\n",
       "      <td>0.045943</td>\n",
       "      <td>0.034284</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>272</td>\n",
       "      <td>1.323906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7625</th>\n",
       "      <td>2016-08-29</td>\n",
       "      <td>CBP_WQX-LE2.3</td>\n",
       "      <td>0.006326</td>\n",
       "      <td>0.014176</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>-0.000951</td>\n",
       "      <td>-0.011854</td>\n",
       "      <td>242</td>\n",
       "      <td>0.705265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7626 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date             StationID  b02_median  b03_median  b04_median  \\\n",
       "0    2025-03-22         USGS_05549500    0.018301    0.032668    0.027823   \n",
       "1    2019-03-05  21VASWCB-2-JMS157.28    0.030897    0.060766    0.073974   \n",
       "2    2022-07-22         UMC-7314/13.6    0.016177    0.022933    0.013090   \n",
       "3    2025-03-25   31DELRBC_WQX-892077    0.016124    0.021928    0.020052   \n",
       "4    2022-10-20  21VASWCB-2-JMS110.30    0.014645    0.030327    0.026220   \n",
       "...         ...                   ...         ...         ...         ...   \n",
       "7621 2023-07-27         USGS_05586300    0.028349    0.051749    0.041979   \n",
       "7622 2024-06-05  USGS_453027122400000    0.016115    0.027910    0.018125   \n",
       "7623 2025-07-23         USGS_05474500    0.029470    0.051300    0.045450   \n",
       "7624 2020-09-28             SCEQ-GL12    0.031900    0.057943    0.051518   \n",
       "7625 2016-08-29         CBP_WQX-LE2.3    0.006326    0.014176    0.004412   \n",
       "\n",
       "      b05_median  b07_median  b8a_median  b09_median  doy  log_chl_a  \n",
       "0       0.033722    0.020919    0.017786    0.024645   81   1.467460  \n",
       "1       0.071041    0.038501    0.027774    0.018974   64   0.176091  \n",
       "2       0.013148    0.028313    0.026328    0.002226  203   0.146128  \n",
       "3       0.015140    0.009258    0.007346    0.000445   84   0.600973  \n",
       "4       0.049796    0.067900    0.071667    0.035572  293  -0.107905  \n",
       "...          ...         ...         ...         ...  ...        ...  \n",
       "7621    0.051625    0.073117    0.064321    0.002565  208   1.287802  \n",
       "7622    0.016156    0.020404    0.018554    0.002418  157  -0.086186  \n",
       "7623    0.060928    0.068560    0.056490    0.002195  204   1.081347  \n",
       "7624    0.064819    0.045943    0.034284    0.001566  272   1.323906  \n",
       "7625    0.000467    0.000937   -0.000951   -0.011854  242   0.705265  \n",
       "\n",
       "[7626 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlss30_insitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d7ec5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hp, hp_space in hyperparameters.items():\n",
    "    for value in hp_space:\n",
    "        model = RandomForestRegressor(**{hp: value, 'random_state': seed})\n",
    "\n",
    "        cv_splitter = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "        # cv_train_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_train_results['hyperparameter'] = []\n",
    "        # cv_train_results['hypervalue'] = []\n",
    "        # cv_train_results['set_type'] = []\n",
    "        # cv_train_results['mission'] = []\n",
    "\n",
    "        # cv_val_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_val_results['hyperparameter'] = []\n",
    "        # cv_val_results['hypervalue'] = []\n",
    "        # cv_val_results['set_type'] = []\n",
    "        # cv_val_results['mission'] = []\n",
    "\n",
    "        for train_index, val_index in cv_splitter.split(hlss30_insitu):\n",
    "            X_train = hlss30_insitu.iloc[train_index][hlss30_features]\n",
    "            y_train = hlss30_insitu.iloc[train_index][hlss30_target]\n",
    "\n",
    "            X_val = hlss30_insitu.iloc[val_index][hlss30_features]\n",
    "            y_val = hlss30_insitu.iloc[val_index][hlss30_target]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            train_metrics = RegressionMetric(\n",
    "                y_true=y_train.values,\n",
    "                y_pred=list(y_train_pred)\n",
    "            )\n",
    "\n",
    "            val_metrics = RegressionMetric(\n",
    "                y_true=y_val.values,\n",
    "                y_pred=list(y_val_pred)\n",
    "            )\n",
    "\n",
    "            train_metrics = train_metrics.get_metrics_by_list_names(list_metrics)\n",
    "            val_metrics = val_metrics.get_metrics_by_list_names(list_metrics)\n",
    "\n",
    "            # calculate metrics\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(train_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('train')\n",
    "            tuning_results['mission'].append('s30')\n",
    "\n",
    "\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(val_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('val')\n",
    "            tuning_results['mission'].append('s30')\n",
    "\n",
    "del hlss30_insitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bc644bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results_df = pd.DataFrame(tuning_results)\n",
    "tuning_results_df.to_csv(f'{hp_name}_results.csv', index=False)\n",
    "\n",
    "del tuning_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26cb2b3",
   "metadata": {},
   "source": [
    "# Min Sample Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc0b7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from thorr.utils import read_config\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import KFold, ShuffleSplit, RepeatedKFold, train_test_split, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from permetrics.regression import RegressionMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8d1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path('/Users/gdarkwah/Library/CloudStorage/OneDrive-UW/01-Research/03-HAB/.env/hab_config.ini')\n",
    "config_dict = read_config(config_path)\n",
    "project_dir = Path(config_dict[\"project\"][\"project_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94b96787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "seed = 1955\n",
    "test_size = 0.2\n",
    "list_metrics = [\"RMSE\", \"MAE\", \"NSE\", \"R2\", \"KGE\", \"MSE\"]\n",
    "\n",
    "# Cross-validation parameters\n",
    "n_splits = 5\n",
    "n_repeats = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57dc9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary for tuning results\n",
    "tuning_results = {metric: [] for metric in list_metrics}\n",
    "tuning_results['hyperparameter'] = []\n",
    "tuning_results['hypervalue'] = []\n",
    "tuning_results['mission'] = []\n",
    "tuning_results['set_type'] = []\n",
    "\n",
    "tuning_results_df = pd.DataFrame(tuning_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aedb7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_name = 'min_samples_split'\n",
    "hp_values = [2, 5] + list(range(10, 101, 10)) + list(range(125, 201, 25)) + list(range(250, 301, 25))\n",
    "\n",
    "hyperparameters = {\n",
    "    hp_name: hp_values,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795c27d",
   "metadata": {},
   "source": [
    "## HLS Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35dfc307",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlsl30_model_name = \"rfr_hlsl30\"\n",
    "\n",
    "hlsl30_features = [\n",
    "    # \"b01_median\",\n",
    "    \"b02_median\",\n",
    "    \"b03_median\",\n",
    "    \"b04_median\",\n",
    "    \"b05_median\",\n",
    "    \"b06_median\",\n",
    "    \"b07_median\",\n",
    "    # \"b09_median\",\n",
    "    \"doy\"\n",
    "]\n",
    "hlsl30_target = \"log_chl_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a80b8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_insitu = pd.read_csv(project_dir / 'data/hls_insitu/hls_insitu.csv', low_memory=False)\n",
    "hls_insitu['Date'] = pd.to_datetime(hls_insitu['Date'])\n",
    "hls_insitu['log_chl_a'] = np.log10(hls_insitu['chl_a'])\n",
    "hls_insitu['doy'] = hls_insitu['Date'].dt.dayofyear\n",
    "\n",
    "# filter l30 data and select features + target\n",
    "hlsl30_insitu = hls_insitu[hls_insitu['mission']=='l30'][\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"StationID\",\n",
    "    ] + hlsl30_features + [hlsl30_target]\n",
    "].copy()\n",
    "\n",
    "# remove hls_insitu from memory\n",
    "del hls_insitu\n",
    "\n",
    "hlsl30_insitu.dropna(\n",
    "    subset=hlsl30_features + [hlsl30_target],\n",
    "    inplace=True,\n",
    "    # how=\"all\"\n",
    ")\n",
    "# shuffle l30 data\n",
    "hlsl30_insitu = hlsl30_insitu.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "759510f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hp, hp_space in hyperparameters.items():\n",
    "    for value in hp_space:\n",
    "        model = RandomForestRegressor(**{hp: value, 'random_state': seed})\n",
    "\n",
    "        cv_splitter = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "        # cv_train_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_train_results['hyperparameter'] = []\n",
    "        # cv_train_results['hypervalue'] = []\n",
    "        # cv_train_results['set_type'] = []\n",
    "        # cv_train_results['mission'] = []\n",
    "\n",
    "        # cv_val_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_val_results['hyperparameter'] = []\n",
    "        # cv_val_results['hypervalue'] = []\n",
    "        # cv_val_results['set_type'] = []\n",
    "        # cv_val_results['mission'] = []\n",
    "\n",
    "        for train_index, val_index in cv_splitter.split(hlsl30_insitu):\n",
    "            X_train = hlsl30_insitu.iloc[train_index][hlsl30_features]\n",
    "            y_train = hlsl30_insitu.iloc[train_index][hlsl30_target]\n",
    "\n",
    "            X_val = hlsl30_insitu.iloc[val_index][hlsl30_features]\n",
    "            y_val = hlsl30_insitu.iloc[val_index][hlsl30_target]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            train_metrics = RegressionMetric(\n",
    "                y_true=y_train.values,\n",
    "                y_pred=list(y_train_pred)\n",
    "            )\n",
    "\n",
    "            val_metrics = RegressionMetric(\n",
    "                y_true=y_val.values,\n",
    "                y_pred=list(y_val_pred)\n",
    "            )\n",
    "\n",
    "            train_metrics = train_metrics.get_metrics_by_list_names(list_metrics)\n",
    "            val_metrics = val_metrics.get_metrics_by_list_names(list_metrics)\n",
    "\n",
    "            # calculate metrics\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(train_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('train')\n",
    "            tuning_results['mission'].append('l30')\n",
    "\n",
    "\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(val_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('val')\n",
    "            tuning_results['mission'].append('l30')\n",
    "\n",
    "del hlsl30_insitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be2322a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results_df = pd.DataFrame(tuning_results)\n",
    "tuning_results_df.to_csv(f'{hp_name}_results.csv', index=False)\n",
    "\n",
    "del tuning_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82571a2a",
   "metadata": {},
   "source": [
    "## HLS Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c70a5f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlss30_model_name = \"rfr_hlss30\"\n",
    "\n",
    "hlss30_features = [\n",
    "    # \"b01_median\",\n",
    "    \"b02_median\",\n",
    "    \"b03_median\",\n",
    "    \"b04_median\",\n",
    "    \"b05_median\",\n",
    "    # \"b06_median\",\n",
    "    \"b07_median\",\n",
    "    # \"b08_median\",\n",
    "    \"b8a_median\",\n",
    "    \"b09_median\",\n",
    "    # \"b10_median\",\n",
    "    # \"b11_median\",\n",
    "    # \"b12_median\",\n",
    "    \"doy\"\n",
    "]\n",
    "hlss30_target = \"log_chl_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2dbe8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_insitu = pd.read_csv(project_dir / 'data/hls_insitu/hls_insitu.csv', low_memory=False)\n",
    "hls_insitu['Date'] = pd.to_datetime(hls_insitu['Date'])\n",
    "hls_insitu['log_chl_a'] = np.log10(hls_insitu['chl_a'])\n",
    "hls_insitu['doy'] = hls_insitu['Date'].dt.dayofyear\n",
    "\n",
    "# filter s30 data and select features + target\n",
    "hlss30_insitu = hls_insitu[hls_insitu['mission']=='s30'][\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"StationID\",\n",
    "    ] + hlss30_features + [hlss30_target]\n",
    "].copy()\n",
    "\n",
    "# remove hls_insitu from memory\n",
    "del hls_insitu\n",
    "\n",
    "hlss30_insitu.dropna(\n",
    "    subset=hlss30_features + [hlss30_target],\n",
    "    inplace=True,\n",
    "    # how=\"all\"\n",
    ")\n",
    "# shuffle l30 data\n",
    "hlss30_insitu = hlss30_insitu.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c533122",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hp, hp_space in hyperparameters.items():\n",
    "    for value in hp_space:\n",
    "        model = RandomForestRegressor(**{hp: value, 'random_state': seed})\n",
    "\n",
    "        cv_splitter = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "        # cv_train_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_train_results['hyperparameter'] = []\n",
    "        # cv_train_results['hypervalue'] = []\n",
    "        # cv_train_results['set_type'] = []\n",
    "        # cv_train_results['mission'] = []\n",
    "\n",
    "        # cv_val_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_val_results['hyperparameter'] = []\n",
    "        # cv_val_results['hypervalue'] = []\n",
    "        # cv_val_results['set_type'] = []\n",
    "        # cv_val_results['mission'] = []\n",
    "\n",
    "        for train_index, val_index in cv_splitter.split(hlss30_insitu):\n",
    "            X_train = hlss30_insitu.iloc[train_index][hlss30_features]\n",
    "            y_train = hlss30_insitu.iloc[train_index][hlss30_target]\n",
    "\n",
    "            X_val = hlss30_insitu.iloc[val_index][hlss30_features]\n",
    "            y_val = hlss30_insitu.iloc[val_index][hlss30_target]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            train_metrics = RegressionMetric(\n",
    "                y_true=y_train.values,\n",
    "                y_pred=list(y_train_pred)\n",
    "            )\n",
    "\n",
    "            val_metrics = RegressionMetric(\n",
    "                y_true=y_val.values,\n",
    "                y_pred=list(y_val_pred)\n",
    "            )\n",
    "\n",
    "            train_metrics = train_metrics.get_metrics_by_list_names(list_metrics)\n",
    "            val_metrics = val_metrics.get_metrics_by_list_names(list_metrics)\n",
    "\n",
    "            # calculate metrics\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(train_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('train')\n",
    "            tuning_results['mission'].append('s30')\n",
    "\n",
    "\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(val_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('val')\n",
    "            tuning_results['mission'].append('s30')\n",
    "\n",
    "del hlss30_insitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65e4cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results_df = pd.DataFrame(tuning_results)\n",
    "tuning_results_df.to_csv(f'{hp_name}_results.csv', index=False)\n",
    "\n",
    "del tuning_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b5ca09",
   "metadata": {},
   "source": [
    "# Min Sample Leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e8c9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from thorr.utils import read_config\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import KFold, ShuffleSplit, RepeatedKFold, train_test_split, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from permetrics.regression import RegressionMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9313fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path('/Users/gdarkwah/Library/CloudStorage/OneDrive-UW/01-Research/03-HAB/.env/hab_config.ini')\n",
    "config_dict = read_config(config_path)\n",
    "project_dir = Path(config_dict[\"project\"][\"project_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "653a79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "seed = 1955\n",
    "test_size = 0.2\n",
    "list_metrics = [\"RMSE\", \"MAE\", \"NSE\", \"R2\", \"KGE\", \"MSE\"]\n",
    "\n",
    "# Cross-validation parameters\n",
    "n_splits = 5\n",
    "n_repeats = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25e58fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary for tuning results\n",
    "tuning_results = {metric: [] for metric in list_metrics}\n",
    "tuning_results['hyperparameter'] = []\n",
    "tuning_results['hypervalue'] = []\n",
    "tuning_results['mission'] = []\n",
    "tuning_results['set_type'] = []\n",
    "\n",
    "tuning_results_df = pd.DataFrame(tuning_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59f401f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_name = 'min_samples_leaf'\n",
    "hp_values = [1] + list(range(10, 101, 10)) + list(range(125, 201, 25)) + list(range(250, 301, 25))\n",
    "\n",
    "hyperparameters = {\n",
    "    hp_name: hp_values,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74454a5e",
   "metadata": {},
   "source": [
    "## HLS Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3704ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlsl30_model_name = \"rfr_hlsl30\"\n",
    "\n",
    "hlsl30_features = [\n",
    "    # \"b01_median\",\n",
    "    \"b02_median\",\n",
    "    \"b03_median\",\n",
    "    \"b04_median\",\n",
    "    \"b05_median\",\n",
    "    \"b06_median\",\n",
    "    \"b07_median\",\n",
    "    # \"b09_median\",\n",
    "    \"doy\"\n",
    "]\n",
    "hlsl30_target = \"log_chl_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5830fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_insitu = pd.read_csv(project_dir / 'data/hls_insitu/hls_insitu.csv', low_memory=False)\n",
    "hls_insitu['Date'] = pd.to_datetime(hls_insitu['Date'])\n",
    "hls_insitu['log_chl_a'] = np.log10(hls_insitu['chl_a'])\n",
    "hls_insitu['doy'] = hls_insitu['Date'].dt.dayofyear\n",
    "\n",
    "# filter l30 data and select features + target\n",
    "hlsl30_insitu = hls_insitu[hls_insitu['mission']=='l30'][\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"StationID\",\n",
    "    ] + hlsl30_features + [hlsl30_target]\n",
    "].copy()\n",
    "\n",
    "# remove hls_insitu from memory\n",
    "del hls_insitu\n",
    "\n",
    "hlsl30_insitu.dropna(\n",
    "    subset=hlsl30_features + [hlsl30_target],\n",
    "    inplace=True,\n",
    "    # how=\"all\"\n",
    ")\n",
    "# shuffle l30 data\n",
    "hlsl30_insitu = hlsl30_insitu.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ec063f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hp, hp_space in hyperparameters.items():\n",
    "    for value in hp_space:\n",
    "        model = RandomForestRegressor(**{hp: value, 'random_state': seed})\n",
    "\n",
    "        cv_splitter = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "        # cv_train_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_train_results['hyperparameter'] = []\n",
    "        # cv_train_results['hypervalue'] = []\n",
    "        # cv_train_results['set_type'] = []\n",
    "        # cv_train_results['mission'] = []\n",
    "\n",
    "        # cv_val_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_val_results['hyperparameter'] = []\n",
    "        # cv_val_results['hypervalue'] = []\n",
    "        # cv_val_results['set_type'] = []\n",
    "        # cv_val_results['mission'] = []\n",
    "\n",
    "        for train_index, val_index in cv_splitter.split(hlsl30_insitu):\n",
    "            X_train = hlsl30_insitu.iloc[train_index][hlsl30_features]\n",
    "            y_train = hlsl30_insitu.iloc[train_index][hlsl30_target]\n",
    "\n",
    "            X_val = hlsl30_insitu.iloc[val_index][hlsl30_features]\n",
    "            y_val = hlsl30_insitu.iloc[val_index][hlsl30_target]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            train_metrics = RegressionMetric(\n",
    "                y_true=y_train.values,\n",
    "                y_pred=list(y_train_pred)\n",
    "            )\n",
    "\n",
    "            val_metrics = RegressionMetric(\n",
    "                y_true=y_val.values,\n",
    "                y_pred=list(y_val_pred)\n",
    "            )\n",
    "\n",
    "            train_metrics = train_metrics.get_metrics_by_list_names(list_metrics)\n",
    "            val_metrics = val_metrics.get_metrics_by_list_names(list_metrics)\n",
    "\n",
    "            # calculate metrics\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(train_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('train')\n",
    "            tuning_results['mission'].append('l30')\n",
    "\n",
    "\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(val_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('val')\n",
    "            tuning_results['mission'].append('l30')\n",
    "\n",
    "del hlsl30_insitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11bff28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results_df = pd.DataFrame(tuning_results)\n",
    "tuning_results_df.to_csv(f'{hp_name}_results.csv', index=False)\n",
    "\n",
    "del tuning_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba82b5",
   "metadata": {},
   "source": [
    "## HLS Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daf1efe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlss30_model_name = \"rfr_hlss30\"\n",
    "\n",
    "hlss30_features = [\n",
    "    # \"b01_median\",\n",
    "    \"b02_median\",\n",
    "    \"b03_median\",\n",
    "    \"b04_median\",\n",
    "    \"b05_median\",\n",
    "    # \"b06_median\",\n",
    "    \"b07_median\",\n",
    "    # \"b08_median\",\n",
    "    \"b8a_median\",\n",
    "    \"b09_median\",\n",
    "    # \"b10_median\",\n",
    "    # \"b11_median\",\n",
    "    # \"b12_median\",\n",
    "    \"doy\"\n",
    "]\n",
    "hlss30_target = \"log_chl_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7317cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_insitu = pd.read_csv(project_dir / 'data/hls_insitu/hls_insitu.csv', low_memory=False)\n",
    "hls_insitu['Date'] = pd.to_datetime(hls_insitu['Date'])\n",
    "hls_insitu['log_chl_a'] = np.log10(hls_insitu['chl_a'])\n",
    "hls_insitu['doy'] = hls_insitu['Date'].dt.dayofyear\n",
    "\n",
    "# filter s30 data and select features + target\n",
    "hlss30_insitu = hls_insitu[hls_insitu['mission']=='s30'][\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"StationID\",\n",
    "    ] + hlss30_features + [hlss30_target]\n",
    "].copy()\n",
    "\n",
    "# remove hls_insitu from memory\n",
    "del hls_insitu\n",
    "\n",
    "hlss30_insitu.dropna(\n",
    "    subset=hlss30_features + [hlss30_target],\n",
    "    inplace=True,\n",
    "    # how=\"all\"\n",
    ")\n",
    "# shuffle l30 data\n",
    "hlss30_insitu = hlss30_insitu.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ca14e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hp, hp_space in hyperparameters.items():\n",
    "    for value in hp_space:\n",
    "        model = RandomForestRegressor(**{hp: value, 'random_state': seed})\n",
    "\n",
    "        cv_splitter = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "        # cv_train_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_train_results['hyperparameter'] = []\n",
    "        # cv_train_results['hypervalue'] = []\n",
    "        # cv_train_results['set_type'] = []\n",
    "        # cv_train_results['mission'] = []\n",
    "\n",
    "        # cv_val_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_val_results['hyperparameter'] = []\n",
    "        # cv_val_results['hypervalue'] = []\n",
    "        # cv_val_results['set_type'] = []\n",
    "        # cv_val_results['mission'] = []\n",
    "\n",
    "        for train_index, val_index in cv_splitter.split(hlss30_insitu):\n",
    "            X_train = hlss30_insitu.iloc[train_index][hlss30_features]\n",
    "            y_train = hlss30_insitu.iloc[train_index][hlss30_target]\n",
    "\n",
    "            X_val = hlss30_insitu.iloc[val_index][hlss30_features]\n",
    "            y_val = hlss30_insitu.iloc[val_index][hlss30_target]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            train_metrics = RegressionMetric(\n",
    "                y_true=y_train.values,\n",
    "                y_pred=list(y_train_pred)\n",
    "            )\n",
    "\n",
    "            val_metrics = RegressionMetric(\n",
    "                y_true=y_val.values,\n",
    "                y_pred=list(y_val_pred)\n",
    "            )\n",
    "\n",
    "            train_metrics = train_metrics.get_metrics_by_list_names(list_metrics)\n",
    "            val_metrics = val_metrics.get_metrics_by_list_names(list_metrics)\n",
    "\n",
    "            # calculate metrics\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(train_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('train')\n",
    "            tuning_results['mission'].append('s30')\n",
    "\n",
    "\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(val_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('val')\n",
    "            tuning_results['mission'].append('s30')\n",
    "\n",
    "del hlss30_insitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3c17032",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results_df = pd.DataFrame(tuning_results)\n",
    "tuning_results_df.to_csv(f'{hp_name}_results.csv', index=False)\n",
    "\n",
    "del tuning_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a4b42",
   "metadata": {},
   "source": [
    "# Number of Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbf872fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from thorr.utils import read_config\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import KFold, ShuffleSplit, RepeatedKFold, train_test_split, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from permetrics.regression import RegressionMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87d9b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path('/Users/gdarkwah/Library/CloudStorage/OneDrive-UW/01-Research/03-HAB/.env/hab_config.ini')\n",
    "config_dict = read_config(config_path)\n",
    "project_dir = Path(config_dict[\"project\"][\"project_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f73ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "seed = 1955\n",
    "test_size = 0.2\n",
    "list_metrics = [\"RMSE\", \"MAE\", \"NSE\", \"R2\", \"KGE\", \"MSE\"]\n",
    "\n",
    "# Cross-validation parameters\n",
    "n_splits = 5\n",
    "n_repeats = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1604dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary for tuning results\n",
    "tuning_results = {metric: [] for metric in list_metrics}\n",
    "tuning_results['hyperparameter'] = []\n",
    "tuning_results['hypervalue'] = []\n",
    "tuning_results['mission'] = []\n",
    "tuning_results['set_type'] = []\n",
    "\n",
    "tuning_results_df = pd.DataFrame(tuning_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "706e6837",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_name = 'n_estimators'\n",
    "hp_values = list(range(10, 101, 10)) + list(range(125, 201, 25)) + list(range(250, 501, 50)) + list(range(600, 1001, 100))\n",
    "\n",
    "hyperparameters = {\n",
    "    hp_name: hp_values,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d5e56",
   "metadata": {},
   "source": [
    "## HLS Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50eb66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlsl30_model_name = \"rfr_hlsl30\"\n",
    "\n",
    "hlsl30_features = [\n",
    "    # \"b01_median\",\n",
    "    \"b02_median\",\n",
    "    \"b03_median\",\n",
    "    \"b04_median\",\n",
    "    \"b05_median\",\n",
    "    \"b06_median\",\n",
    "    \"b07_median\",\n",
    "    # \"b09_median\",\n",
    "    \"doy\"\n",
    "]\n",
    "hlsl30_target = \"log_chl_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8bbaed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_insitu = pd.read_csv(project_dir / 'data/hls_insitu/hls_insitu.csv', low_memory=False)\n",
    "hls_insitu['Date'] = pd.to_datetime(hls_insitu['Date'])\n",
    "hls_insitu['log_chl_a'] = np.log10(hls_insitu['chl_a'])\n",
    "hls_insitu['doy'] = hls_insitu['Date'].dt.dayofyear\n",
    "\n",
    "# filter l30 data and select features + target\n",
    "hlsl30_insitu = hls_insitu[hls_insitu['mission']=='l30'][\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"StationID\",\n",
    "    ] + hlsl30_features + [hlsl30_target]\n",
    "].copy()\n",
    "\n",
    "# remove hls_insitu from memory\n",
    "del hls_insitu\n",
    "\n",
    "hlsl30_insitu.dropna(\n",
    "    subset=hlsl30_features + [hlsl30_target],\n",
    "    inplace=True,\n",
    "    # how=\"all\"\n",
    ")\n",
    "# shuffle l30 data\n",
    "hlsl30_insitu = hlsl30_insitu.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44837a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hp, hp_space in hyperparameters.items():\n",
    "    for value in hp_space:\n",
    "        model = RandomForestRegressor(**{hp: value, 'random_state': seed})\n",
    "\n",
    "        cv_splitter = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "        # cv_train_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_train_results['hyperparameter'] = []\n",
    "        # cv_train_results['hypervalue'] = []\n",
    "        # cv_train_results['set_type'] = []\n",
    "        # cv_train_results['mission'] = []\n",
    "\n",
    "        # cv_val_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_val_results['hyperparameter'] = []\n",
    "        # cv_val_results['hypervalue'] = []\n",
    "        # cv_val_results['set_type'] = []\n",
    "        # cv_val_results['mission'] = []\n",
    "\n",
    "        for train_index, val_index in cv_splitter.split(hlsl30_insitu):\n",
    "            X_train = hlsl30_insitu.iloc[train_index][hlsl30_features]\n",
    "            y_train = hlsl30_insitu.iloc[train_index][hlsl30_target]\n",
    "\n",
    "            X_val = hlsl30_insitu.iloc[val_index][hlsl30_features]\n",
    "            y_val = hlsl30_insitu.iloc[val_index][hlsl30_target]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            train_metrics = RegressionMetric(\n",
    "                y_true=y_train.values,\n",
    "                y_pred=list(y_train_pred)\n",
    "            )\n",
    "\n",
    "            val_metrics = RegressionMetric(\n",
    "                y_true=y_val.values,\n",
    "                y_pred=list(y_val_pred)\n",
    "            )\n",
    "\n",
    "            train_metrics = train_metrics.get_metrics_by_list_names(list_metrics)\n",
    "            val_metrics = val_metrics.get_metrics_by_list_names(list_metrics)\n",
    "\n",
    "            # calculate metrics\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(train_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('train')\n",
    "            tuning_results['mission'].append('l30')\n",
    "\n",
    "\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(val_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('val')\n",
    "            tuning_results['mission'].append('l30')\n",
    "\n",
    "del hlsl30_insitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37cade79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results_df = pd.DataFrame(tuning_results)\n",
    "tuning_results_df.to_csv(f'{hp_name}_results.csv', index=False)\n",
    "\n",
    "del tuning_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a1a0b",
   "metadata": {},
   "source": [
    "## HLS Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d318316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlss30_model_name = \"rfr_hlss30\"\n",
    "\n",
    "hlss30_features = [\n",
    "    # \"b01_median\",\n",
    "    \"b02_median\",\n",
    "    \"b03_median\",\n",
    "    \"b04_median\",\n",
    "    \"b05_median\",\n",
    "    # \"b06_median\",\n",
    "    \"b07_median\",\n",
    "    # \"b08_median\",\n",
    "    \"b8a_median\",\n",
    "    \"b09_median\",\n",
    "    # \"b10_median\",\n",
    "    # \"b11_median\",\n",
    "    # \"b12_median\",\n",
    "    \"doy\"\n",
    "]\n",
    "hlss30_target = \"log_chl_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d365309",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_insitu = pd.read_csv(project_dir / 'data/hls_insitu/hls_insitu.csv', low_memory=False)\n",
    "hls_insitu['Date'] = pd.to_datetime(hls_insitu['Date'])\n",
    "hls_insitu['log_chl_a'] = np.log10(hls_insitu['chl_a'])\n",
    "hls_insitu['doy'] = hls_insitu['Date'].dt.dayofyear\n",
    "\n",
    "# filter s30 data and select features + target\n",
    "hlss30_insitu = hls_insitu[hls_insitu['mission']=='s30'][\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"StationID\",\n",
    "    ] + hlss30_features + [hlss30_target]\n",
    "].copy()\n",
    "\n",
    "# remove hls_insitu from memory\n",
    "del hls_insitu\n",
    "\n",
    "hlss30_insitu.dropna(\n",
    "    subset=hlss30_features + [hlss30_target],\n",
    "    inplace=True,\n",
    "    # how=\"all\"\n",
    ")\n",
    "# shuffle l30 data\n",
    "hlss30_insitu = hlss30_insitu.sample(frac=1, random_state=seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69d6415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hp, hp_space in hyperparameters.items():\n",
    "    for value in hp_space:\n",
    "        model = RandomForestRegressor(**{hp: value, 'random_state': seed})\n",
    "\n",
    "        cv_splitter = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "\n",
    "        # cv_train_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_train_results['hyperparameter'] = []\n",
    "        # cv_train_results['hypervalue'] = []\n",
    "        # cv_train_results['set_type'] = []\n",
    "        # cv_train_results['mission'] = []\n",
    "\n",
    "        # cv_val_results = {metric: [] for metric in list_metrics}\n",
    "        # cv_val_results['hyperparameter'] = []\n",
    "        # cv_val_results['hypervalue'] = []\n",
    "        # cv_val_results['set_type'] = []\n",
    "        # cv_val_results['mission'] = []\n",
    "\n",
    "        for train_index, val_index in cv_splitter.split(hlss30_insitu):\n",
    "            X_train = hlss30_insitu.iloc[train_index][hlss30_features]\n",
    "            y_train = hlss30_insitu.iloc[train_index][hlss30_target]\n",
    "\n",
    "            X_val = hlss30_insitu.iloc[val_index][hlss30_features]\n",
    "            y_val = hlss30_insitu.iloc[val_index][hlss30_target]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            train_metrics = RegressionMetric(\n",
    "                y_true=y_train.values,\n",
    "                y_pred=list(y_train_pred)\n",
    "            )\n",
    "\n",
    "            val_metrics = RegressionMetric(\n",
    "                y_true=y_val.values,\n",
    "                y_pred=list(y_val_pred)\n",
    "            )\n",
    "\n",
    "            train_metrics = train_metrics.get_metrics_by_list_names(list_metrics)\n",
    "            val_metrics = val_metrics.get_metrics_by_list_names(list_metrics)\n",
    "\n",
    "            # calculate metrics\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(train_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('train')\n",
    "            tuning_results['mission'].append('s30')\n",
    "\n",
    "\n",
    "            for metric in list_metrics:\n",
    "                tuning_results[metric].append(val_metrics[metric])\n",
    "\n",
    "            tuning_results['hyperparameter'].append(hp)\n",
    "            tuning_results['hypervalue'].append(value)\n",
    "            tuning_results['set_type'].append('val')\n",
    "            tuning_results['mission'].append('s30')\n",
    "\n",
    "del hlss30_insitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59503890",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results_df = pd.DataFrame(tuning_results)\n",
    "tuning_results_df.to_csv(f'{hp_name}_results.csv', index=False)\n",
    "\n",
    "del tuning_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
