{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68a43e6",
   "metadata": {},
   "source": [
    "# Monte Carlo Ranom Forest + Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecd3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from thorr.utils import read_config\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import KFold, ShuffleSplit, RepeatedKFold, train_test_split, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet, LinearRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from permetrics.regression import RegressionMetric\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d78976",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path('/Users/gdarkwah/Library/CloudStorage/OneDrive-UW/01-Research/03-HAB/.env/hab_config.ini')\n",
    "config_dict = read_config(config_path)\n",
    "project_dir = Path(config_dict[\"project\"][\"project_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0202109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('/Volumes/STLP-0800/monte_carlo_rfr_base/')\n",
    "dev_results_dir = results_dir / 'dev_results'\n",
    "dev_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "dev_models_dir = results_dir / 'dev_models'\n",
    "dev_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_results_dir = results_dir / 'test_results'\n",
    "test_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "final_models_dir = results_dir / 'final_models'\n",
    "final_models_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da0f60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "seed = 1955\n",
    "test_size = 0.2\n",
    "list_metrics = [\"RMSE\", \"MAE\", \"NSE\", \"R2\", \"KGE\", \"MSE\", \"MAPE\"]\n",
    "\n",
    "# Cross-validation parameters\n",
    "n_splits = 5\n",
    "n_repeats = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc41afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_insitu = pd.read_csv(project_dir / 'data/hls_insitu/hls_insitu_chl_a_clean.csv', low_memory=False)\n",
    "hls_insitu['Date'] = pd.to_datetime(hls_insitu['Date'])\n",
    "hls_insitu['log_chl_a'] = np.log10(hls_insitu['chl_a'])\n",
    "hls_insitu['doy'] = hls_insitu['Date'].dt.dayofyear\n",
    "\n",
    "# hls_insitu.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411b7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev, test = train_test_split(hls_insitu, test_size=test_size, random_state=seed)\n",
    "# test = pd.concat([test, joliet_data]).reset_index(drop=True)\n",
    "\n",
    "del hls_insitu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925eb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev.to_csv('dev_set.csv', index=False)\n",
    "# test.to_csv('test_set.csv', index=False)\n",
    "\n",
    "dev = pd.read_csv('dev_set.csv')\n",
    "test = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c918b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReachID\n",
       "12028    682\n",
       "12029    632\n",
       "11563    572\n",
       "11634    432\n",
       "6928     428\n",
       "        ... \n",
       "5803       1\n",
       "9653       1\n",
       "3359       1\n",
       "12089      1\n",
       "5973       1\n",
       "Name: count, Length: 1412, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['ReachID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9766bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlsl30_model_name = \"rfr_hlsl30\"\n",
    "\n",
    "hlsl30_features = [\n",
    "    # \"b01_median\",\n",
    "    \"b02_median\",\n",
    "    \"b03_median\",\n",
    "    \"b04_median\",\n",
    "    # \"b05_median\",\n",
    "    \"b06_median\",\n",
    "    # \"b07_median\",\n",
    "    # \"b09_median\",\n",
    "    \"doy\"\n",
    "]\n",
    "hlsl30_target = \"log_chl_a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d78c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlss30_model_name = \"rfr_hlss30\"\n",
    "\n",
    "hlss30_features = [\n",
    "    # \"b01_median\",\n",
    "    \"b02_median\",\n",
    "    \"b03_median\",\n",
    "    \"b04_median\",\n",
    "    \"b05_median\",\n",
    "    # \"b06_median\",\n",
    "    \"b07_median\",\n",
    "    # \"b08_median\",\n",
    "    \"b8a_median\",\n",
    "    \"b09_median\",\n",
    "    # \"b10_median\",\n",
    "    # \"b11_median\",\n",
    "    # \"b12_median\",\n",
    "    \"doy\"\n",
    "]\n",
    "hlss30_target = \"log_chl_a\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d97c9e",
   "metadata": {},
   "source": [
    "## Development (Dev)\n",
    "Run the algorithm for the training and validation sets (using the dev set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc474bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_minority_class(X, y, target_col, target_bins, random_state=None):\n",
    "    oversampled_data = X.copy()\n",
    "    oversampled_data[target_col] = y\n",
    "\n",
    "    oversampling_bins = target_bins\n",
    "    oversampled_data['bin'] = pd.cut(oversampled_data[target_col], bins=oversampling_bins)\n",
    "    max_bin_count = round(oversampled_data['bin'].value_counts().max()*2)\n",
    "\n",
    "    oversampled_data_list = []\n",
    "    for bin in oversampled_data['bin'].unique():\n",
    "        bin_data = oversampled_data[oversampled_data['bin'] == bin]\n",
    "        if len(bin_data) >0:\n",
    "            oversampled_data_list.append(bin_data.sample(max_bin_count, replace=True, random_state=random_state))\n",
    "    \n",
    "    oversampled_data = pd.concat(oversampled_data_list).drop(columns=['bin']).reset_index(drop=True)\n",
    "    \n",
    "    return oversampled_data.drop(columns=[target_col]), oversampled_data[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b4191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 100 iterations of Monte Carlo cross-validation.\n",
      "Completed 200 iterations of Monte Carlo cross-validation.\n",
      "Completed 300 iterations of Monte Carlo cross-validation.\n",
      "Completed 400 iterations of Monte Carlo cross-validation.\n",
      "Completed 500 iterations of Monte Carlo cross-validation.\n"
     ]
    }
   ],
   "source": [
    "# Use cross-validation as a Monte Carlo sampler to determine uncertainty in model predictions\n",
    "\n",
    "# cv_splitter = RepeatedKFold(n_splits=5, n_repeats=400, random_state=42)\n",
    "# for i, (train_index, test_index) in enumerate(cv_splitter.split(dev)):\n",
    "for i in range(2000):\n",
    "    dev = dev.sample(frac=1, random_state=seed+i).reset_index(drop=True) # shuffle the data for each iteration\n",
    "    train_index, test_index = train_test_split(np.arange(len(dev)), test_size=test_size, random_state=seed+i)\n",
    "\n",
    "    # l30_train = hls_insitu.iloc[train_index]\n",
    "    # s30_train = hls_insitu.iloc[train_index]\n",
    "    if i+1 >= 0: # set to 0 to run all iterations, or set to a specific number to run a subset of iterations\n",
    "        l30_train = dev.iloc[train_index]\n",
    "        s30_train = dev.iloc[train_index]\n",
    "        \n",
    "        l30_train = l30_train[l30_train['mission'] == 'l30']\n",
    "        s30_train = s30_train[s30_train['mission'] == 's30']\n",
    "\n",
    "        l30_train = l30_train.dropna(subset=hlsl30_features + [hlsl30_target])\n",
    "        s30_train = s30_train.dropna(subset=hlss30_features + [hlss30_target])\n",
    "\n",
    "        X_l30_train = l30_train[hlsl30_features]\n",
    "        y_l30_train = l30_train[hlsl30_target]\n",
    "\n",
    "        oversampled_X_l30_train, oversampled_y_l30_train = oversample_minority_class(X_l30_train, y_l30_train, hlsl30_target, target_bins=np.linspace(-.72, 2.55, 50))\n",
    "\n",
    "        X_s30_train = s30_train[hlss30_features]\n",
    "        y_s30_train = s30_train[hlss30_target]\n",
    "\n",
    "        oversampled_X_s30_train, oversampled_y_s30_train = oversample_minority_class(X_s30_train, y_s30_train, hlss30_target, target_bins=np.linspace(-.72, 2.55, 50))\n",
    "\n",
    "        rfr_l30 = RandomForestRegressor(n_estimators=250, #min_samples_split=30\n",
    "                                        max_features=\"sqrt\"\n",
    "                                        )\n",
    "        rfr_s30 = RandomForestRegressor(n_estimators=250, #min_samples_split=30,\n",
    "                                        max_features=\"sqrt\"\n",
    "                                        )\n",
    "\n",
    "        # rfr_l30.fit(oversampled_X_l30_train, oversampled_y_l30_train)\n",
    "        # rfr_s30.fit(oversampled_X_s30_train, oversampled_y_s30_train)\n",
    "\n",
    "        rfr_l30.fit(X_l30_train, y_l30_train)\n",
    "        rfr_s30.fit(X_s30_train, y_s30_train)\n",
    "\n",
    "        l30_train['pred_log_chl_a'] = rfr_l30.predict(X_l30_train)\n",
    "        s30_train['pred_log_chl_a'] = rfr_s30.predict(X_s30_train)\n",
    "\n",
    "        train_results = pd.concat([l30_train, s30_train], ignore_index=True)\n",
    "        train_results = train_results[['Date', 'ReachID', 'mission', 'chl_a', 'log_chl_a', 'pred_log_chl_a']].copy()\n",
    "        train_results['model_no'] = i+1\n",
    "\n",
    "        train_results.to_csv(dev_results_dir / f'monte_carlo_train_{i+1}.csv', index=False)\n",
    "\n",
    "        del l30_train, s30_train\n",
    "\n",
    "        l30_val = dev.iloc[test_index]\n",
    "        s30_val = dev.iloc[test_index]\n",
    "\n",
    "        l30_val = l30_val[l30_val['mission'] == 'l30']\n",
    "        s30_val = s30_val[s30_val['mission'] == 's30']\n",
    "\n",
    "        l30_val = l30_val.dropna(subset=hlsl30_features + [hlsl30_target])\n",
    "        s30_val = s30_val.dropna(subset=hlss30_features + [hlss30_target])\n",
    "\n",
    "        X_l30_val = l30_val[hlsl30_features]\n",
    "        y_l30_val = l30_val[hlsl30_target]\n",
    "\n",
    "        X_s30_val = s30_val[hlss30_features]\n",
    "        y_s30_val = s30_val[hlss30_target]\n",
    "\n",
    "        y_l30_pred = rfr_l30.predict(X_l30_val)\n",
    "        y_s30_pred = rfr_s30.predict(X_s30_val)\n",
    "\n",
    "        # Store predictions for uncertainty analysis\n",
    "        l30_val['pred_log_chl_a'] = y_l30_pred\n",
    "        s30_val['pred_log_chl_a'] = y_s30_pred\n",
    "\n",
    "        val_results = pd.concat([l30_val, s30_val], ignore_index=True)\n",
    "        val_results = val_results[['Date', 'ReachID', 'mission', 'chl_a', 'log_chl_a', 'pred_log_chl_a']].copy()\n",
    "        val_results['model_no'] = i+1\n",
    "\n",
    "        val_results.to_csv(dev_results_dir / f'monte_carlo_val_{i+1}.csv', index=False)\n",
    "\n",
    "        del l30_val, s30_val\n",
    "\n",
    "        dump(rfr_l30, dev_models_dir / f'rfr_l30_model_{i+1}.joblib')\n",
    "        dump(rfr_s30, dev_models_dir / f'rfr_s30_model_{i+1}.joblib')\n",
    "\n",
    "        del train_results, val_results\n",
    "        del rfr_l30, rfr_s30\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"Completed {i+1} iterations of Monte Carlo cross-validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b3321",
   "metadata": {},
   "source": [
    "## Test\n",
    "Test the algorithm using the test set and the models saaved from the dev mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58d6c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed predictions for 100 models.\n",
      "Completed predictions for 200 models.\n",
      "Completed predictions for 300 models.\n",
      "Completed predictions for 400 models.\n",
      "Completed predictions for 500 models.\n",
      "Completed predictions for 600 models.\n",
      "Completed predictions for 700 models.\n",
      "Completed predictions for 800 models.\n",
      "Completed predictions for 900 models.\n",
      "Completed predictions for 1000 models.\n",
      "Completed predictions for 1100 models.\n",
      "Completed predictions for 1200 models.\n",
      "Completed predictions for 1300 models.\n",
      "Completed predictions for 1400 models.\n",
      "Completed predictions for 1500 models.\n",
      "Completed predictions for 1600 models.\n",
      "Completed predictions for 1700 models.\n",
      "Completed predictions for 1800 models.\n",
      "Completed predictions for 1900 models.\n",
      "Completed predictions for 2000 models.\n",
      "Completed predictions for 2100 models.\n",
      "Completed predictions for 2200 models.\n",
      "Completed predictions for 2300 models.\n",
      "Completed predictions for 2400 models.\n",
      "Completed predictions for 2500 models.\n",
      "Completed predictions for 2600 models.\n",
      "Completed predictions for 2700 models.\n",
      "Completed predictions for 2800 models.\n",
      "Completed predictions for 2900 models.\n",
      "Completed predictions for 3000 models.\n",
      "Completed predictions for 3100 models.\n",
      "Completed predictions for 3200 models.\n",
      "Completed predictions for 3300 models.\n",
      "Completed predictions for 3400 models.\n",
      "Completed predictions for 3500 models.\n",
      "Completed predictions for 3600 models.\n",
      "Completed predictions for 3700 models.\n",
      "Completed predictions for 3800 models.\n",
      "Completed predictions for 3900 models.\n",
      "Completed predictions for 4000 models.\n"
     ]
    }
   ],
   "source": [
    "# saved models\n",
    "dev_models = glob.glob(str(dev_models_dir) + '/*.joblib')\n",
    "n_dev_models = len(dev_models) # combined number of l30 and s30 models\n",
    "\n",
    "test_results_list = []\n",
    "\n",
    "for i, model_file in enumerate(dev_models):\n",
    "    # check if there is l30 or s30 in the filename\n",
    "    if 'l30' in model_file:\n",
    "        model = load(model_file)\n",
    "        test_data = test[test['mission'] == 'l30']\n",
    "    elif 's30' in model_file:\n",
    "        model = load(model_file)\n",
    "        test_data = test[test['mission'] == 's30']\n",
    "    \n",
    "    test_data = test_data.dropna(subset=hlsl30_features + [hlsl30_target]) if 'l30' in model_file else test_data.dropna(subset=hlss30_features + [hlss30_target])\n",
    "\n",
    "    X_test = test_data[hlsl30_features] if 'l30' in model_file else test_data[hlss30_features]\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_data['pred_log_chl_a'] = y_pred\n",
    "    \n",
    "    # get model number from filename\n",
    "    model_no = int(model_file.split('_')[-1].split('.')[0])\n",
    "    test_data['model_no'] = model_no\n",
    "\n",
    "    test_results_list.append(test_data[['Date', 'ReachID', 'mission', 'chl_a', 'log_chl_a', 'pred_log_chl_a', 'model_no']])\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"Completed predictions for {i+1} models.\")\n",
    "\n",
    "test_results_df = pd.concat(test_results_list, ignore_index=True)\n",
    "test_results_df.to_csv(test_results_dir / 'monte_carlo_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847eacc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
